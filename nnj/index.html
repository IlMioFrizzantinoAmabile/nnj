


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Overview &mdash; nnj 0.0.0 documentation</title>
  

<!--   <link rel="shortcut icon" href="../_static/images/logo192.png" /> -->
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/readthedocs.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="Why to use nnj?" href="../why/index.html" />
  <link rel="prev" title="NNJ Documentation" href="../index.html" />
  <!-- Google Analytics -->
  <script type="text/javascript">
    var collapsedSections = [];
  </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href=""
        aria-label="LOGO"></a>

      <div class="main-menu">
        <ul>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Learn about nnj</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="#jacobian-in-a-neural-network">Jacobian in a neural network</a></li>
<li class="toctree-l1"><a class="reference internal" href="#efficient-implementation">Efficient implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="#getting-started">Getting Started</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Ok, but why?</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../why/index.html">Why to use nnj?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../why/index.html#why-not-jax">Why not Jax?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../why/index.html#comparison">Comparison</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../apis/abstract_diagonal_jacobian.html">Abstract Jacobian (diagonal)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/abstract_jacobian.html">Abstract Jacobian</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/linear.html">Linear</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/maxpool.html">MaxPool2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/relu.html">ReLU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/sequential.html">Sequential</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/sigmoid.html">Sigmoid</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/sinusoidal.html">Sinusoidal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/softplus.html">Softplus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/tanh.html">Tanh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/truncexp.html">TruncExp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/upsample.html">Upsample</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
            Docs
        </a> &gt;
      </li>

        
      <li>Overview</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/nnj/index.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="overview">
<span id="introduction"></span><h1>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h1>
<p>A function <span class="math notranslate nohighlight">\(f: \mathcal{X}\rightarrow\mathcal{Y}\)</span> induces a map between directions <span class="math notranslate nohighlight">\(v\in\mathcal{X}\)</span> to directions <span class="math notranslate nohighlight">\(w\in\mathcal{Y}\)</span> such that <span class="math notranslate nohighlight">\(f(x + v) = f(x) + w\)</span>, when the step is <em>small</em>.
This map is known as the derivative <span class="math notranslate nohighlight">\(\nabla f\)</span> and, when evaluated in a specific point <span class="math notranslate nohighlight">\(\nabla f: x\in\mathcal{X}\)</span>, it is a linear operator <span class="math notranslate nohighlight">\(\nabla f(x): T_x\mathcal{X}\rightarrow T_{f(x)}\mathcal{Y}\)</span> between the tangent space in <span class="math notranslate nohighlight">\(x\)</span> and the tangent space in <span class="math notranslate nohighlight">\(f(x)\)</span>.</p>
<p>The linear operator is just a matrix multiplication, the matrix is called <strong>Jacobian</strong> and it is usually identified with the same notation as the linear operator itself <span class="math notranslate nohighlight">\(J=\nabla f(x)\)</span>.</p>
<p>The forward <em>direction</em> pass <strong>JVP</strong> (Jacobian Vector Product) is</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
J \cdot v \in  T_{f(x)}\mathcal{Y}
\qquad\qquad \forall v\in  T_x\mathcal{X}
\end{aligned}\]</div>
<p>The backward <em>direction</em> pass <strong>VJP</strong> (Vector Jacobian Product) is</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
J^\top \cdot w \in  T_x\mathcal{X}
\qquad \forall w\in  T_{f(x)}\mathcal{Y}
\end{aligned}\]</div>
<p>The forward <em>metric</em> pass <strong>JMJtP</strong> (Jacobian Matrix Jacobian transpose Product) is</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
J \cdot M \cdot J^\top \in  \mathfrak{M}(T_{f(x)}\mathcal{Y})
\qquad\qquad \forall M\in \mathfrak{M}(T_x\mathcal{X})
\end{aligned}\]</div>
<p>The backward <em>metric</em> pass <strong>JtMJP</strong> (Jacobian transpose Matrix Jacobian Product) is</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
J^\top \cdot M \cdot J \in  \mathfrak{M}(T_x\mathcal{X})
\qquad\qquad \forall M\in \mathfrak{M}(T_{f(x)}\mathcal{Y})
\end{aligned}\]</div>
<p>The first two jacobian products are more common and supported by several repos (like JAX), while the latter are more specific and not commonly supported.
The scope of this repository is to provide efficent implementation of these jacobian products for all PyTorch <em>modules</em>.
In PyTorch, objects of the class <em>module</em> are the building blocks for functions, they can be either parametric layer (as linear or convolutions) or non-parametric layer (as tanh or relu).
Moreover, composition of modules is a module itself (sequential) and even arbitrary function of other modules can be a module (as skip-connection or res-block or attention).</p>
</section>
<section id="jacobian-in-a-neural-network">
<h1>Jacobian in a neural network<a class="headerlink" href="#jacobian-in-a-neural-network" title="Permalink to this heading">¶</a></h1>
<p>A neural network is actually a function with two inputs: data and parameters. Formally</p>
<div class="math notranslate nohighlight">
\[f: \mathcal{X}\times\Theta\rightarrow\mathcal{Y}\]</div>
<dl class="simple">
<dt>And thus, depending on which input we take the derivative with respct to, there are two different Jacobian to consider:</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(J=\nabla_x f(x,\theta)\)</span> when computed with respect to the data</p></li>
<li><p><span class="math notranslate nohighlight">\(J=\nabla_\theta f(x,\theta)\)</span> when computed with respect to parameter</p></li>
</ul>
</dd>
</dl>
<p>Following the common usage of neural networks, we use the keyword <em>input</em> for data and <em>weight</em> for parameter.</p>
</section>
<section id="efficient-implementation">
<h1>Efficient implementation<a class="headerlink" href="#efficient-implementation" title="Permalink to this heading">¶</a></h1>
<p>The building block for neural networks, as in PyTorch, is the <strong>Module</strong>: a black-box function with two inputs: data and parameters. Such black-box can either be defined <em>explicitly</em> or <em>implicitly</em>.</p>
<dl class="simple">
<dt>Explicitly defined modules further split into:</dt><dd><ul class="simple">
<li><p>parametric layer (as linear or convolutions)</p></li>
<li><p>non-parametric layer (as tanh or relu)</p></li>
</ul>
</dd>
<dt>Implicitly defined modules can be:</dt><dd><ul class="simple">
<li><p>composition of other modules ( <a class="reference internal" href="../apis/sequential.html#sequential"><span class="std std-ref">Introduction to sequential</span></a> )</p></li>
<li><p>arbitrary function of other modules (as skip-connection or res-block or attention)</p></li>
</ul>
</dd>
</dl>
<p>Efficient implementation of Jacobian products are based on different levels of abstraction.
Explicitly defined modules allow access to the explicit form of the Jacobian product and this allows to implement them <strong>without ever instantiating the full Jacobian matrix</strong> (which is memory consuming and practically often means storing a bunch of useless zeros).
Implicitly defined modules, instead, rely on the chain rule and on the efficient implementation of the Jacobian product of the building blocks.</p>
</section>
<section id="getting-started">
<h1>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this heading">¶</a></h1>
<p>Defining a nnj neural network can be done either directly as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnj</span>

<span class="c1"># Define you sequential model in nnj</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nnj</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
   <span class="n">nnj</span><span class="o">.</span><span class="n">Linear</span><span class="p">(),</span>
   <span class="n">nnj</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
   <span class="n">nnh</span><span class="o">.</span><span class="n">Linear</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>or through standard torch.nn and convertion</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnj</span>

<span class="c1"># Define you sequential model in torch.nn</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
   <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(),</span>
   <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
   <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(),</span>
<span class="p">)</span>

<span class="c1"># convert to nnj</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nnj</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">convert_to_nnj</span><span class="p">(</span>
   <span class="n">model</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>And computing jacobian products is as simple as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">val</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Compute gradient (of the l2 loss) as backward pass of the residual</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">val</span> <span class="o">-</span> <span class="n">target</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">vjp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">wrt</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">)</span>

<span class="c1"># Compute the Generalized-Gauss Newton (an approximation of the hessian) as a backward pass of the Euclidean metric</span>
<span class="n">jacobianTranspose_jacobian</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_jTmjp</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">val</span><span class="p">,</span>
    <span class="kc">None</span><span class="p">,</span>               <span class="c1"># None means identity (i.e. Euclidean metric)</span>
    <span class="n">wrt</span><span class="o">=</span><span class="s2">&quot;weights&quot;</span><span class="p">,</span>      <span class="c1"># computes the jacobian wrt weights or inputs</span>
    <span class="n">to_diag</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>       <span class="c1"># computes the diagonal elements only</span>
    <span class="n">diag_backprop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># approximates the diagonal elements, which speeds up the computations</span>
<span class="p">)</span>

<span class="c1"># Average along batch size</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">jacobianTranspose_jacobian</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jacobianTranspose_jacobian</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="../why/index.html" class="btn btn-neutral float-right" title="Why to use nnj?" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="../index.html" class="btn btn-neutral" title="NNJ Documentation" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2023, Marco Miani and Frederik Warburg.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Overview</a></li>
<li><a class="reference internal" href="#jacobian-in-a-neural-network">Jacobian in a neural network</a></li>
<li><a class="reference internal" href="#efficient-implementation">Efficient implementation</a></li>
<li><a class="reference internal" href="#getting-started">Getting Started</a></li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  <!-- 
 -->

  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="../_static/doctools.js"></script>
  <script src="../_static/sphinx_highlight.js"></script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="" aria-label="LOGO"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>